<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Posts on Terminal</title>
    <link>/posts/</link>
    <description>Recent content in Posts on Terminal</description>
    <generator>Hugo</generator>
    <language>en</language>
    <lastBuildDate>Wed, 02 Apr 2025 00:00:00 +0000</lastBuildDate>
    <atom:link href="/posts/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>Continual Learning</title>
      <link>/posts/continual-learning/</link>
      <pubDate>Wed, 02 Apr 2025 00:00:00 +0000</pubDate>
      <guid>/posts/continual-learning/</guid>
      <description>&lt;p&gt;**&lt;/p&gt;&#xA;&lt;h1 id=&#34;continual-learning-a-review-of-variational-dropout-mixture-of-experts-with-prompting-and-backdoor-attacks&#34;&gt;Continual Learning: A Review of Variational Dropout, Mixture of Experts with Prompting, and Backdoor Attacks&lt;/h1&gt;&#xA;&lt;h2 id=&#34;1-introduction&#34;&gt;1. Introduction&lt;/h2&gt;&#xA;&lt;p&gt;The field of machine learning has witnessed significant advancements in recent years, enabling models to achieve remarkable performance on a wide array of tasks. However, a fundamental challenge arises when these models are deployed in dynamic environments where new data or tasks are encountered sequentially. This paradigm, known as continual learning, necessitates the ability of a model to learn from a continuous stream of information without forgetting previously acquired knowledge.1 A major impediment to achieving this goal is catastrophic forgetting, a phenomenon where the learning of new information leads to a drastic decline in performance on previously learned tasks.4 Overcoming this challenge requires specialized techniques that can maintain a delicate balance between the model&amp;rsquo;s capacity to learn new tasks (plasticity) and its ability to retain old knowledge (stability).4&lt;/p&gt;</description>
    </item>
    <item>
      <title>list</title>
      <link>/posts/list-of-researchers-to-follow-up/</link>
      <pubDate>Tue, 01 Apr 2025 00:00:00 +0000</pubDate>
      <guid>/posts/list-of-researchers-to-follow-up/</guid>
      <description>&lt;h1 id=&#34;yann-lecun-the-future-beyond-generative-ai&#34;&gt;Yann LeCun: The Future Beyond Generative AI&lt;/h1&gt;&#xA;&lt;p&gt;12 sources&lt;/p&gt;&#xA;&lt;p&gt;The provided transcripts capture various discussions and lectures primarily focusing on the evolution, capabilities, limitations, and societal implications of artificial intelligence and deep learning. &lt;strong&gt;Experts like Geoffrey Hinton, Yann LeCun, and Fei-Fei Li reflect on breakthroughs such as deep neural networks and large language models, including their own significant contributions.&lt;/strong&gt; They discuss the future trajectory of AI research, highlighting the importance of world models, different learning approaches like joint embedding, and the distinctions between human and artificial intelligence. &lt;strong&gt;Concerns surrounding responsible AI development, potential misuse, and the need for open-source platforms are also prominent themes.&lt;/strong&gt; Additionally, personal anecdotes about the speakers&amp;rsquo; journeys and perspectives on the field enrich the content. &lt;strong&gt;The conversations explore both the technical advancements and the broader philosophical and ethical questions raised by increasingly sophisticated AI.&lt;/strong&gt;&lt;/p&gt;</description>
    </item>
    <item>
      <title>Moe-JEPA vs Titan vs FAN</title>
      <link>/posts/moe-jepa-vs-titan-vs-fan/</link>
      <pubDate>Tue, 01 Apr 2025 00:00:00 +0000</pubDate>
      <guid>/posts/moe-jepa-vs-titan-vs-fan/</guid>
      <description>&lt;p&gt;make a markdown code about the following content:&lt;/p&gt;&#xA;&lt;h1 id=&#34;comparative-analysis-of-advanced-ai-architectures-fourier-analysis-networks-google-titan-transformer-20-and-moe-jepa-world-models&#34;&gt;Comparative Analysis of Advanced AI Architectures: Fourier Analysis Networks, Google Titan Transformer 2.0, and MoE-JEPA World Models&lt;/h1&gt;&#xA;&lt;p&gt;The field of artificial intelligence has experienced remarkable evolution with several novel architectures emerging to address the limitations of conventional deep learning approaches. This research provides a comprehensive comparative analysis of three cutting-edge AI architectures: Fourier Analysis Networks (FANs), Google Titan Transformer 2.0, and Mixture of Experts Joint Embedding Predictive Architecture (MoE-JEPA) World Models. Each model employs distinct approaches to overcome current AI limitations, particularly in handling periodic structures, long-term dependencies, and context understanding. Through detailed examination of their architectures, operational mechanisms, advantages, limitations, and empirical performance, this study offers insights into their potential impact on the future trajectory of artificial intelligence research and applications.&lt;/p&gt;</description>
    </item>
    <item>
      <title>Hugo Terminal Template</title>
      <link>/posts/hello-world/</link>
      <pubDate>Mon, 31 Mar 2025 00:00:00 +0000</pubDate>
      <guid>/posts/hello-world/</guid>
      <description>&lt;p&gt;Start&#xA;Credit to:&#xA;Hugo Terminal Template&lt;/p&gt;&#xA;&lt;p&gt;&lt;img src=&#34;https://github.com/panr/hugo-theme-terminal/blob/master/images/terminal-theme.png?raw=true&#34; alt=&#34;Terminal&#34;&gt;&lt;/p&gt;&#xA;&lt;hr&gt;</description>
    </item>
  </channel>
</rss>
