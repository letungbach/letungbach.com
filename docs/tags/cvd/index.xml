<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Cvd on Le Tung Bach, Ph.D.</title>
    <link>https://letungbach.com/tags/cvd/</link>
    <description>Recent content in Cvd on Le Tung Bach, Ph.D.</description>
    <generator>Hugo</generator>
    <language>en</language>
    <lastBuildDate>Mon, 14 Apr 2025 00:00:00 +0000</lastBuildDate>
    <atom:link href="https://letungbach.com/tags/cvd/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>ZhangXu</title>
      <link>https://letungbach.com/posts/zhangxu/</link>
      <pubDate>Mon, 14 Apr 2025 00:00:00 +0000</pubDate>
      <guid>https://letungbach.com/posts/zhangxu/</guid>
      <description>&lt;p&gt;**&lt;/p&gt;&#xA;&lt;h1 id=&#34;zhang-xu-innovation-and-expressiveness-in-tang-dynasty-calligraphy&#34;&gt;Zhang Xu: Innovation and Expressiveness in Tang Dynasty Calligraphy&lt;/h1&gt;&#xA;&lt;figure&gt;&lt;img src=&#34;https://upload.wikimedia.org/wikipedia/commons/4/48/Crazyzhangxu.jpg&#34;&gt;&lt;figcaption&gt;&#xA;      &lt;h4&gt;ZhangXu&lt;/h4&gt;&#xA;    &lt;/figcaption&gt;&#xA;&lt;/figure&gt;&#xA;&#xA;&lt;ol&gt;&#xA;&lt;li&gt;Abstract&lt;/li&gt;&#xA;&lt;/ol&gt;&#xA;&lt;p&gt;Zhang Xu (張旭, fl. 8th century), courtesy name Bogao (伯高), stands as a seminal figure in the history of Chinese calligraphy, particularly celebrated for his revolutionary and highly expressive &amp;ldquo;wild cursive&amp;rdquo; (狂草) style. This report provides a comprehensive examination of Zhang Xu&amp;rsquo;s life and artistic contributions. It encompasses a detailed biography, an in-depth analysis of the unique characteristics of his calligraphic style, a review of existing academic literature concerning his work, an identification of gaps in current research, a formulation of research objectives and a problem statement, and proposals for future studies that could further illuminate his significance. The enduring impact of Zhang Xu&amp;rsquo;s innovative approach to calligraphy and his lasting influence on subsequent generations of calligraphers are also briefly considered.&lt;/p&gt;</description>
    </item>
    <item>
      <title>market-research</title>
      <link>https://letungbach.com/posts/market-research/</link>
      <pubDate>Sun, 13 Apr 2025 00:00:00 +0000</pubDate>
      <guid>https://letungbach.com/posts/market-research/</guid>
      <description>&lt;p&gt;**&lt;/p&gt;&#xA;&lt;h1 id=&#34;market-opportunities-for-ai-agents-and-multi-ai-agent-systems-in-vietnam&#34;&gt;Market Opportunities for AI Agents and Multi-AI Agent Systems in Vietnam&lt;/h1&gt;&#xA;&lt;h2 id=&#34;1-executive-summary&#34;&gt;1. Executive Summary&lt;/h2&gt;&#xA;&lt;p&gt;Vietnam&amp;rsquo;s digital landscape is undergoing a rapid transformation, presenting significant opportunities for the adoption of advanced automation technologies such as AI Agents and Multi-AI Agent systems. This report provides a comprehensive analysis of the Vietnamese market, highlighting the immediate needs across key sectors including travel tourism, real estate, customer service, logistics, and manufacturing. The analysis reveals a strong government commitment to digital transformation and AI development, coupled with a high rate of technology adoption among businesses. While the market for AI Agents and Multi-AI Agent systems is still in its early stages, specific areas like customer service automation, personalized experiences in travel tourism, and efficiency improvements in real estate show immediate promise. This report recommends a phased approach to market entry, initially focusing on these high-potential areas with tailored strategies for marketing, pricing, and customer acquisition, ultimately positioning service providers to capitalize on the transformative power of AI in Vietnam.&lt;/p&gt;</description>
    </item>
    <item>
      <title>New_LLM</title>
      <link>https://letungbach.com/posts/new-model-introduction/</link>
      <pubDate>Thu, 10 Apr 2025 00:00:00 +0000</pubDate>
      <guid>https://letungbach.com/posts/new-model-introduction/</guid>
      <description>&lt;p&gt;LLM model introduction&lt;/p&gt;&#xA;&lt;p&gt;&lt;a href=&#34;https://allenai.org/blog/olmotrace&#34;&gt;https://allenai.org/blog/olmotrace&lt;/a&gt;&lt;/p&gt;</description>
    </item>
    <item>
      <title>Continual Learning</title>
      <link>https://letungbach.com/posts/continual-learning/</link>
      <pubDate>Wed, 02 Apr 2025 00:00:00 +0000</pubDate>
      <guid>https://letungbach.com/posts/continual-learning/</guid>
      <description>&lt;p&gt;**&lt;/p&gt;&#xA;&lt;h1 id=&#34;continual-learning-a-review-of-variational-dropout-mixture-of-experts-with-prompting-and-backdoor-attacks&#34;&gt;Continual Learning: A Review of Variational Dropout, Mixture of Experts with Prompting, and Backdoor Attacks&lt;/h1&gt;&#xA;&lt;h2 id=&#34;1-introduction&#34;&gt;1. Introduction&lt;/h2&gt;&#xA;&lt;p&gt;The field of machine learning has witnessed significant advancements in recent years, enabling models to achieve remarkable performance on a wide array of tasks. However, a fundamental challenge arises when these models are deployed in dynamic environments where new data or tasks are encountered sequentially. This paradigm, known as continual learning, necessitates the ability of a model to learn from a continuous stream of information without forgetting previously acquired knowledge.1 A major impediment to achieving this goal is catastrophic forgetting, a phenomenon where the learning of new information leads to a drastic decline in performance on previously learned tasks.4 Overcoming this challenge requires specialized techniques that can maintain a delicate balance between the model&amp;rsquo;s capacity to learn new tasks (plasticity) and its ability to retain old knowledge (stability).4&lt;/p&gt;</description>
    </item>
    <item>
      <title>benchmark</title>
      <link>https://letungbach.com/posts/benchmark/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://letungbach.com/posts/benchmark/</guid>
      <description>&lt;p&gt;&lt;a href=&#34;https://codeforces.com/blog/entry/133874&#34;&gt;https://codeforces.com/blog/entry/133874&lt;/a&gt;&lt;/p&gt;&#xA;&lt;p&gt;&lt;a href=&#34;https://www.swebench.com/&#34;&gt;https://www.swebench.com/&lt;/a&gt;&lt;/p&gt;&#xA;&lt;p&gt;&lt;a href=&#34;https://linzhiqiu.github.io/papers/naturalbench/?fbclid=IwY2xjawJ1xCpleHRuA2FlbQIxMQABHnFZ6hln8p75Kuz4l9F4Mgow7kzEgS1GKuRYj6q-DlvUAWVVRiyVmW1SvnwQ_aem_y_RMPY4cokQHJk8TpxQwpQ&#34;&gt;https://linzhiqiu.github.io/papers/naturalbench/?fbclid=IwY2xjawJ1xCpleHRuA2FlbQIxMQABHnFZ6hln8p75Kuz4l9F4Mgow7kzEgS1GKuRYj6q-DlvUAWVVRiyVmW1SvnwQ_aem_y_RMPY4cokQHJk8TpxQwpQ&lt;/a&gt;&lt;/p&gt;&#xA;&lt;p&gt;&lt;a href=&#34;https://github.com/Baiqi-Li/NaturalBench&#34;&gt;https://github.com/Baiqi-Li/NaturalBench&lt;/a&gt;&lt;/p&gt;&#xA;&lt;p&gt;trackingAI.org&lt;/p&gt;&#xA;&lt;p&gt;&lt;a href=&#34;https://llm-stats.com/&#34;&gt;https://llm-stats.com/&lt;/a&gt;&lt;/p&gt;&#xA;&lt;p&gt;&lt;a href=&#34;https://lmarena.ai/?leaderboard&#34;&gt;https://lmarena.ai/?leaderboard&lt;/a&gt;&lt;/p&gt;&#xA;&lt;p&gt;&lt;a href=&#34;https://fiction.live/stories/Fiction-liveBench-Feb-21-2025/oQdzQvKHw8JyXbN87&#34;&gt;https://fiction.live/stories/Fiction-liveBench-Feb-21-2025/oQdzQvKHw8JyXbN87&lt;/a&gt;&lt;/p&gt;&#xA;&lt;p&gt;&lt;a href=&#34;https://huggingface.co/spaces/adyen/DABstep&#34;&gt;Data Agent Benchmark&lt;/a&gt;&lt;/p&gt;&#xA;&lt;p&gt;&lt;a href=&#34;https://artificialanalysis.ai/&#34;&gt;https://artificialanalysis.ai/&lt;/a&gt;&lt;/p&gt;&#xA;&lt;p&gt;LiveCodeBench và SciCode&lt;/p&gt;&#xA;&lt;p&gt;&lt;a href=&#34;https://aider.chat/docs/leaderboards/?fbclid=IwY2xjawJs18lleHRuA2FlbQIxMQABHpMzr6OCU0YD65sAyMY5vDd4DKn00s4RKJniEUvlJIIeX4sIYMCtIq7MLZw8_aem_yF2JsBDjLMvkeFDhYfb-6A&#34;&gt;Aider polyglot&lt;/a&gt;&#xA;&lt;a href=&#34;https://github.com/Aider-AI/aider&#34;&gt;github&lt;/a&gt;&lt;/p&gt;&#xA;&lt;p&gt;&lt;a href=&#34;https://livebench.ai&#34;&gt;https://livebench.ai&lt;/a&gt;&#xA;&lt;a href=&#34;https://github.com/LiveBench/LiveBench&#34;&gt;https://github.com/LiveBench/LiveBench&lt;/a&gt;&lt;/p&gt;&#xA;&lt;p&gt;&lt;a href=&#34;https://openrouter.ai/rankings&#34;&gt;https://openrouter.ai/rankings&lt;/a&gt;&lt;/p&gt;&#xA;&lt;p&gt;&lt;a href=&#34;https://arcprize.org/leaderboard?fbclid=IwY2xjawJkGOJleHRuA2FlbQIxMAABHpInxwGwuzaVHnGeNNycEGfhmweu8Xb_aBq5dhGnOHLm1qEbktYZYnqZzNmc_aem_ttSWRTegPXjvOSU1K0DAlg&#34;&gt;https://arcprize.org/leaderboard?fbclid=IwY2xjawJkGOJleHRuA2FlbQIxMAABHpInxwGwuzaVHnGeNNycEGfhmweu8Xb_aBq5dhGnOHLm1qEbktYZYnqZzNmc_aem_ttSWRTegPXjvOSU1K0DAlg&lt;/a&gt;&lt;/p&gt;&#xA;&lt;p&gt;![[Pasted image 20250410103636.png]]&lt;/p&gt;&#xA;&lt;p&gt;EQ-Bench - Longform Creative Writing: &lt;a href=&#34;https://arxiv.org/pdf/2312.06281&#34;&gt;paper&lt;/a&gt;&#xA;![EQ-Bench][https://eqbench.com/images/eqbench3-judge-comparison.png]&lt;/p&gt;&#xA;&lt;p&gt;&lt;a href=&#34;https://llmbenchmark.kili-technology.com/?_gl=1&#34;&gt;https://llmbenchmark.kili-technology.com/?_gl=1&lt;/a&gt;&lt;em&gt;1y0re2j&lt;/em&gt;_gcl_au*NzA4OTAwNjM4LjE3NDQ5MjAzNDE.&lt;/p&gt;&#xA;&lt;figure&gt;&lt;img src=&#34;https://eqbench.com/images/eqbench3-judge-comparison.png&#34;&gt;&lt;figcaption&gt;&#xA;      &lt;h4&gt;Judge Comparison&lt;/h4&gt;&#xA;    &lt;/figcaption&gt;&#xA;&lt;/figure&gt;&#xA;&#xA;&lt;p&gt;Table of Contents&lt;/p&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;&lt;a href=&#34;https://kili-technology.com/large-language-models-llms/9-open-sourced-datasets-for-training-large-language-models#3&#34;&gt;Why are datasets important in training LLMs?&lt;/a&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;&lt;a href=&#34;https://kili-technology.com/large-language-models-llms/9-open-sourced-datasets-for-training-large-language-models#common-challenges-when-preparing-training-datasets&#34;&gt;Common challenges when preparing training datasets&lt;/a&gt;&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;/li&gt;&#xA;&lt;li&gt;&lt;a href=&#34;https://kili-technology.com/large-language-models-llms/9-open-sourced-datasets-for-training-large-language-models#27&#34;&gt;Popular Open Source Datasets for Training LLMs&lt;/a&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;&lt;a href=&#34;https://kili-technology.com/large-language-models-llms/9-open-sourced-datasets-for-training-large-language-models#1.-common-crawl&#34;&gt;1. Common Crawl&lt;/a&gt;&lt;/li&gt;&#xA;&lt;li&gt;&lt;a href=&#34;https://kili-technology.com/large-language-models-llms/9-open-sourced-datasets-for-training-large-language-models#2.-refinedweb&#34;&gt;2. RefinedWeb&lt;/a&gt;&lt;/li&gt;&#xA;&lt;li&gt;&lt;a href=&#34;https://kili-technology.com/large-language-models-llms/9-open-sourced-datasets-for-training-large-language-models#3.-the-pile&#34;&gt;3. The Pile&lt;/a&gt;&lt;/li&gt;&#xA;&lt;li&gt;&lt;a href=&#34;https://kili-technology.com/large-language-models-llms/9-open-sourced-datasets-for-training-large-language-models#4.-c4&#34;&gt;4. C4&lt;/a&gt;&lt;/li&gt;&#xA;&lt;li&gt;&lt;a href=&#34;https://kili-technology.com/large-language-models-llms/9-open-sourced-datasets-for-training-large-language-models#5.-starcoder-data&#34;&gt;5. Starcoder Data&lt;/a&gt;&lt;/li&gt;&#xA;&lt;li&gt;&lt;a href=&#34;https://kili-technology.com/large-language-models-llms/9-open-sourced-datasets-for-training-large-language-models#6.-bookcorpus&#34;&gt;6. BookCorpus&lt;/a&gt;&lt;/li&gt;&#xA;&lt;li&gt;&lt;a href=&#34;https://kili-technology.com/large-language-models-llms/9-open-sourced-datasets-for-training-large-language-models#7.-roots&#34;&gt;7. ROOTS&lt;/a&gt;&lt;/li&gt;&#xA;&lt;li&gt;&lt;a href=&#34;https://kili-technology.com/large-language-models-llms/9-open-sourced-datasets-for-training-large-language-models#8.-wikipedia&#34;&gt;8. Wikipedia&lt;/a&gt;&lt;/li&gt;&#xA;&lt;li&gt;&lt;a href=&#34;https://kili-technology.com/large-language-models-llms/9-open-sourced-datasets-for-training-large-language-models#9.-red-pajama&#34;&gt;9. Red Pajama&lt;/a&gt;&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;/li&gt;&#xA;&lt;li&gt;&lt;a href=&#34;https://kili-technology.com/large-language-models-llms/9-open-sourced-datasets-for-training-large-language-models#73&#34;&gt;Why Data Preprocessing is Important when Using Open Source Datasets&lt;/a&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;&lt;a href=&#34;https://kili-technology.com/large-language-models-llms/9-open-sourced-datasets-for-training-large-language-models#data-pre-processing-techniques&#34;&gt;Data Pre-Processing Techniques&lt;/a&gt;&lt;/li&gt;&#xA;&lt;li&gt;&lt;a href=&#34;https://kili-technology.com/large-language-models-llms/9-open-sourced-datasets-for-training-large-language-models#cleaning-and-normalization&#34;&gt;Cleaning and normalization&lt;/a&gt;&lt;/li&gt;&#xA;&lt;li&gt;&lt;a href=&#34;https://kili-technology.com/large-language-models-llms/9-open-sourced-datasets-for-training-large-language-models#tokenization-and-vectorization&#34;&gt;Tokenization and vectorization&lt;/a&gt;&lt;/li&gt;&#xA;&lt;li&gt;&lt;a href=&#34;https://kili-technology.com/large-language-models-llms/9-open-sourced-datasets-for-training-large-language-models#handling-of-missing-data&#34;&gt;Handling of missing data&lt;/a&gt;&lt;/li&gt;&#xA;&lt;li&gt;&lt;a href=&#34;https://kili-technology.com/large-language-models-llms/9-open-sourced-datasets-for-training-large-language-models#data-augmentation&#34;&gt;Data augmentation&lt;/a&gt;&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;/li&gt;&#xA;&lt;li&gt;&lt;a href=&#34;https://kili-technology.com/large-language-models-llms/9-open-sourced-datasets-for-training-large-language-models#115&#34;&gt;Ethical Considerations and Challenges When Applying Datasets in Machine Learning&lt;/a&gt;&lt;/li&gt;&#xA;&lt;li&gt;&lt;a href=&#34;https://kili-technology.com/large-language-models-llms/9-open-sourced-datasets-for-training-large-language-models#126&#34;&gt;How Datasets are used in Fine-Tuning and Evaluating LLMs&lt;/a&gt;&lt;/li&gt;&#xA;&lt;li&gt;&lt;a href=&#34;https://kili-technology.com/large-language-models-llms/9-open-sourced-datasets-for-training-large-language-models#142&#34;&gt;Conclusion: You Reap what you Sow&lt;/a&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;&lt;a href=&#34;https://kili-technology.com/large-language-models-llms/9-open-sourced-datasets-for-training-large-language-models#additional-reading&#34;&gt;Additional Reading&lt;/a&gt;&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;p&gt;The emergence of large language models (LLMs) sparks revolutionary transformation in various industries. While ChatGPT has impressed the public with its ingenious take on poetic writing, organizations are adopting deep learning AI models to build advanced neural information processing systems for specialized use cases.&lt;/p&gt;</description>
    </item>
    <item>
      <title>LLMtesting</title>
      <link>https://letungbach.com/posts/test/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://letungbach.com/posts/test/</guid>
      <description>&lt;p&gt;&lt;a href=&#34;https://www.promptfoo.dev/&#34;&gt;https://www.promptfoo.dev/&lt;/a&gt;&#xA;&lt;a href=&#34;https://github.com/promptfoo/promptfoo&#34;&gt;https://github.com/promptfoo/promptfoo&lt;/a&gt;&lt;/p&gt;</description>
    </item>
    <item>
      <title>mcp</title>
      <link>https://letungbach.com/posts/mcp/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://letungbach.com/posts/mcp/</guid>
      <description>&lt;p&gt;opensource MCP:&#xA;&lt;a href=&#34;https://github.com/pietrozullo/mcp-use&#34;&gt;https://github.com/pietrozullo/mcp-use&lt;/a&gt;&lt;/p&gt;&#xA;&lt;p&gt;![[Pasted image 20250414230045.png]]![[Pasted image 20250414230109.png]]&lt;/p&gt;&#xA;&lt;p&gt;![[Pasted image 20250416205343.png]]&lt;/p&gt;&#xA;&lt;p&gt;  &amp;ldquo;my-mcp-server-fdbeb09b&amp;rdquo;: {&lt;/p&gt;&#xA;&lt;p&gt;                &amp;ldquo;type&amp;rdquo;: &amp;ldquo;stdio&amp;rdquo;,&lt;/p&gt;&#xA;&lt;p&gt;                &amp;ldquo;command&amp;rdquo;: &amp;ldquo;@modelcontextprotocol/docker-mcp&amp;rdquo;,&lt;/p&gt;&#xA;&lt;p&gt;                &amp;ldquo;args&amp;rdquo;: []&lt;/p&gt;&#xA;&lt;p&gt;            }&lt;/p&gt;</description>
    </item>
  </channel>
</rss>
