<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Neuralnet on Le Tung Bach, Ph.D.</title>
    <link>https://letungbach.com/tags/neuralnet/</link>
    <description>Recent content in Neuralnet on Le Tung Bach, Ph.D.</description>
    <generator>Hugo</generator>
    <language>en</language>
    <lastBuildDate>Mon, 14 Apr 2025 00:00:00 +0000</lastBuildDate>
    <atom:link href="https://letungbach.com/tags/neuralnet/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>ZhangXu</title>
      <link>https://letungbach.com/posts/zhangxu/</link>
      <pubDate>Mon, 14 Apr 2025 00:00:00 +0000</pubDate>
      <guid>https://letungbach.com/posts/zhangxu/</guid>
      <description>&lt;p&gt;**&lt;/p&gt;&#xA;&lt;h1 id=&#34;zhang-xu-innovation-and-expressiveness-in-tang-dynasty-calligraphy&#34;&gt;Zhang Xu: Innovation and Expressiveness in Tang Dynasty Calligraphy&lt;/h1&gt;&#xA;&lt;figure&gt;&lt;img src=&#34;https://upload.wikimedia.org/wikipedia/commons/4/48/Crazyzhangxu.jpg&#34;&gt;&lt;figcaption&gt;&#xA;      &lt;h4&gt;ZhangXu&lt;/h4&gt;&#xA;    &lt;/figcaption&gt;&#xA;&lt;/figure&gt;&#xA;&#xA;&lt;ol&gt;&#xA;&lt;li&gt;Abstract&lt;/li&gt;&#xA;&lt;/ol&gt;&#xA;&lt;p&gt;Zhang Xu (張旭, fl. 8th century), courtesy name Bogao (伯高), stands as a seminal figure in the history of Chinese calligraphy, particularly celebrated for his revolutionary and highly expressive &amp;ldquo;wild cursive&amp;rdquo; (狂草) style. This report provides a comprehensive examination of Zhang Xu&amp;rsquo;s life and artistic contributions. It encompasses a detailed biography, an in-depth analysis of the unique characteristics of his calligraphic style, a review of existing academic literature concerning his work, an identification of gaps in current research, a formulation of research objectives and a problem statement, and proposals for future studies that could further illuminate his significance. The enduring impact of Zhang Xu&amp;rsquo;s innovative approach to calligraphy and his lasting influence on subsequent generations of calligraphers are also briefly considered.&lt;/p&gt;</description>
    </item>
    <item>
      <title>market-research</title>
      <link>https://letungbach.com/posts/market-research/</link>
      <pubDate>Sun, 13 Apr 2025 00:00:00 +0000</pubDate>
      <guid>https://letungbach.com/posts/market-research/</guid>
      <description>&lt;p&gt;**&lt;/p&gt;&#xA;&lt;h1 id=&#34;market-opportunities-for-ai-agents-and-multi-ai-agent-systems-in-vietnam&#34;&gt;Market Opportunities for AI Agents and Multi-AI Agent Systems in Vietnam&lt;/h1&gt;&#xA;&lt;h2 id=&#34;1-executive-summary&#34;&gt;1. Executive Summary&lt;/h2&gt;&#xA;&lt;p&gt;Vietnam&amp;rsquo;s digital landscape is undergoing a rapid transformation, presenting significant opportunities for the adoption of advanced automation technologies such as AI Agents and Multi-AI Agent systems. This report provides a comprehensive analysis of the Vietnamese market, highlighting the immediate needs across key sectors including travel tourism, real estate, customer service, logistics, and manufacturing. The analysis reveals a strong government commitment to digital transformation and AI development, coupled with a high rate of technology adoption among businesses. While the market for AI Agents and Multi-AI Agent systems is still in its early stages, specific areas like customer service automation, personalized experiences in travel tourism, and efficiency improvements in real estate show immediate promise. This report recommends a phased approach to market entry, initially focusing on these high-potential areas with tailored strategies for marketing, pricing, and customer acquisition, ultimately positioning service providers to capitalize on the transformative power of AI in Vietnam.&lt;/p&gt;</description>
    </item>
    <item>
      <title>New_LLM</title>
      <link>https://letungbach.com/posts/new-model-introduction/</link>
      <pubDate>Thu, 10 Apr 2025 00:00:00 +0000</pubDate>
      <guid>https://letungbach.com/posts/new-model-introduction/</guid>
      <description>&lt;p&gt;LLM model introduction&lt;/p&gt;&#xA;&lt;p&gt;&lt;a href=&#34;https://allenai.org/blog/olmotrace&#34;&gt;https://allenai.org/blog/olmotrace&lt;/a&gt;&lt;/p&gt;</description>
    </item>
    <item>
      <title>Self-Rag</title>
      <link>https://letungbach.com/posts/self-rag/</link>
      <pubDate>Thu, 10 Apr 2025 00:00:00 +0000</pubDate>
      <guid>https://letungbach.com/posts/self-rag/</guid>
      <description>&lt;p&gt;**&lt;/p&gt;&#xA;&lt;h1 id=&#34;advancing-agentic-knowledgeable-self-awareness-a-research-agenda-extending-arxiv250403553&#34;&gt;Advancing Agentic Knowledgeable Self-Awareness: A Research Agenda Extending arXiv:2504.03553&lt;/h1&gt;&#xA;&lt;h2 id=&#34;1-introduction&#34;&gt;1. Introduction&lt;/h2&gt;&#xA;&lt;p&gt;The development of artificial intelligence (AI) agents capable of complex tasks necessitates mechanisms for robust and efficient knowledge utilization. A critical aspect of this is self-awareness regarding the agent&amp;rsquo;s own knowledge state – understanding what it knows, what it doesn&amp;rsquo;t know, and when external information is required. The paper arXiv:2504.03553 introduces the concept of &amp;ldquo;agentic knowledgeable self-awareness&amp;rdquo; and proposes the &amp;ldquo;KnowSelf&amp;rdquo; method as a novel approach to instill this capability in language agents. KnowSelf utilizes special tokens and a two-stage training process to explicitly signal the agent&amp;rsquo;s perceived knowledge state and guide its information processing strategy (e.g., relying on internal parameters vs. seeking external knowledge).&lt;/p&gt;</description>
    </item>
    <item>
      <title>Continual Learning</title>
      <link>https://letungbach.com/posts/continual-learning/</link>
      <pubDate>Wed, 02 Apr 2025 00:00:00 +0000</pubDate>
      <guid>https://letungbach.com/posts/continual-learning/</guid>
      <description>&lt;p&gt;**&lt;/p&gt;&#xA;&lt;h1 id=&#34;continual-learning-a-review-of-variational-dropout-mixture-of-experts-with-prompting-and-backdoor-attacks&#34;&gt;Continual Learning: A Review of Variational Dropout, Mixture of Experts with Prompting, and Backdoor Attacks&lt;/h1&gt;&#xA;&lt;h2 id=&#34;1-introduction&#34;&gt;1. Introduction&lt;/h2&gt;&#xA;&lt;p&gt;The field of machine learning has witnessed significant advancements in recent years, enabling models to achieve remarkable performance on a wide array of tasks. However, a fundamental challenge arises when these models are deployed in dynamic environments where new data or tasks are encountered sequentially. This paradigm, known as continual learning, necessitates the ability of a model to learn from a continuous stream of information without forgetting previously acquired knowledge.1 A major impediment to achieving this goal is catastrophic forgetting, a phenomenon where the learning of new information leads to a drastic decline in performance on previously learned tasks.4 Overcoming this challenge requires specialized techniques that can maintain a delicate balance between the model&amp;rsquo;s capacity to learn new tasks (plasticity) and its ability to retain old knowledge (stability).4&lt;/p&gt;</description>
    </item>
    <item>
      <title>MoE-JEPA</title>
      <link>https://letungbach.com/posts/moe-jepa/</link>
      <pubDate>Mon, 31 Mar 2025 00:00:00 +0000</pubDate>
      <guid>https://letungbach.com/posts/moe-jepa/</guid>
      <description>&lt;h1 id=&#34;research-proposal-moe-jepa-world-models-for-efficient-reinforcement-learning-and-planning&#34;&gt;Research Proposal: MoE-JEPA World Models for Efficient Reinforcement Learning and Planning&lt;/h1&gt;&#xA;&lt;h2 id=&#34;abstract&#34;&gt;Abstract&lt;/h2&gt;&#xA;&lt;p&gt;Current AI research emphasizes the development of sophisticated world models capable of understanding complex dynamics, particularly from video data, often leveraging self-supervised learning (SSL) for representation extraction. Predictive models in abstract spaces (like JEPA) are gaining prominence over generative ones. Simultaneously, Mixture of Experts (MoE) offers a way to scale neural network capacity efficiently. This proposal outlines a research approach combining these trends: developing an Action-Conditioned Mixture-of-Experts Joint-Embedding Predictive Architecture (MoE-JEPA) world model. This model will be pre-trained using self-supervision on large video datasets to learn robust visual representations and environment dynamics. The MoE structure will allow the model to efficiently capture diverse or multi-modal dynamics within an environment by routing inputs to specialized expert sub-networks. This sophisticated world model will then be integrated into a model-based Reinforcement Learning (RL) framework to enable efficient planning and decision-making for agents (e.g., robots) interacting with complex environments. We hypothesize that this approach will lead to more accurate world models, improved sample efficiency in RL, and better generalization across tasks compared to monolithic world models.&lt;/p&gt;</description>
    </item>
    <item>
      <title>AI visibility</title>
      <link>https://letungbach.com/posts/ai-optimized-search-engine/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://letungbach.com/posts/ai-optimized-search-engine/</guid>
      <description>&lt;p&gt;Optimizing your website source code for AI search engines involves tailoring your content and structure to be easily accessible and understandable by AI systems. Here are some key strategies:&lt;/p&gt;&#xA;&lt;h3 id=&#34;1-use-clean-html-and-markdown&#34;&gt;&lt;strong&gt;1. Use Clean HTML and Markdown&lt;/strong&gt;&lt;/h3&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;Ensure your website uses clean, well-structured HTML or Markdown. Avoid excessive JavaScript, as many AI crawlers struggle with it.&lt;/li&gt;&#xA;&lt;li&gt;Logical content hierarchy (headings, paragraphs, lists) helps AI systems parse your content effectively.&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;h3 id=&#34;2-implement-semantic-markup&#34;&gt;&lt;strong&gt;2. Implement Semantic Markup&lt;/strong&gt;&lt;/h3&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;Use semantic HTML tags like &lt;code&gt;&amp;lt;article&amp;gt;&lt;/code&gt;, &lt;code&gt;&amp;lt;section&amp;gt;&lt;/code&gt;, &lt;code&gt;&amp;lt;header&amp;gt;&lt;/code&gt;, and &lt;code&gt;&amp;lt;footer&amp;gt;&lt;/code&gt; to provide context about your content.&lt;/li&gt;&#xA;&lt;li&gt;Add metadata and structured data using &lt;a href=&#34;https://searchengineland.com/ai-optimization-how-to-optimize-your-content-for-ai-search-and-agents-451287&#34;&gt;Schema.org&lt;/a&gt; to help AI understand your site&amp;rsquo;s purpose and content.&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;h3 id=&#34;3-create-an&#34;&gt;&lt;strong&gt;3. Create an &lt;code&gt;/llms.txt&lt;/code&gt; File&lt;/strong&gt;&lt;/h3&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;Include an &lt;code&gt;/llms.txt&lt;/code&gt; file on your website to provide AI-specific metadata and summaries. This file acts as a guide for AI systems to navigate your site efficiently.&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;h3 id=&#34;4-optimize-for-speed&#34;&gt;&lt;strong&gt;4. Optimize for Speed&lt;/strong&gt;&lt;/h3&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;AI systems often have tight timeouts for retrieving content. Optimize your website&amp;rsquo;s loading speed by:&#xA;&lt;ul&gt;&#xA;&lt;li&gt;Compressing images.&lt;/li&gt;&#xA;&lt;li&gt;Minifying CSS and JavaScript.&lt;/li&gt;&#xA;&lt;li&gt;Using a Content Delivery Network (CDN).&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;h3 id=&#34;5-configure-robotstxt&#34;&gt;&lt;strong&gt;5. Configure Robots.txt&lt;/strong&gt;&lt;/h3&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;Allow AI crawlers access to your site by configuring your &lt;code&gt;robots.txt&lt;/code&gt; file. For example:&#xA;&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-plaintext&#34; data-lang=&#34;plaintext&#34;&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;User-agent: OAI-SearchBot&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;Allow: /&#xA;&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/li&gt;&#xA;&lt;li&gt;Be mindful of which bots you allow, as some may collect training data.&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;h3 id=&#34;6-prioritize-key-information&#34;&gt;&lt;strong&gt;6. Prioritize Key Information&lt;/strong&gt;&lt;/h3&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;Place important content at the top of your pages to ensure AI systems retrieve it quickly.&lt;/li&gt;&#xA;&lt;li&gt;Use concise and clear language to improve AI&amp;rsquo;s ability to summarize your content.&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;h3 id=&#34;7-test-ai-visibility&#34;&gt;&lt;strong&gt;7. Test AI Visibility&lt;/strong&gt;&lt;/h3&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;Use tools like &lt;a href=&#34;https://searchengineland.com/ai-optimization-how-to-optimize-your-content-for-ai-search-and-agents-451287&#34;&gt;Firecrawl&lt;/a&gt; or AndiSearch to test how AI systems perceive and access your content.&lt;/li&gt;&#xA;&lt;li&gt;Check if your pages are accessible and useful for AI-driven search engines.&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;h3 id=&#34;8-optimize-for-voice-search&#34;&gt;&lt;strong&gt;8. Optimize for Voice Search&lt;/strong&gt;&lt;/h3&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;Create content that answers common questions directly and concisely. AI systems often prioritize content suitable for voice search.&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;p&gt;By implementing these strategies, your website will be better equipped to rank in AI-driven search results and interact effectively with AI systems. Let me know if you&amp;rsquo;d like help with any specific step!&lt;/p&gt;</description>
    </item>
    <item>
      <title>benchmark</title>
      <link>https://letungbach.com/posts/benchmark/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://letungbach.com/posts/benchmark/</guid>
      <description>&lt;p&gt;&lt;a href=&#34;https://codeforces.com/blog/entry/133874&#34;&gt;https://codeforces.com/blog/entry/133874&lt;/a&gt;&lt;/p&gt;&#xA;&lt;p&gt;&lt;a href=&#34;https://www.swebench.com/&#34;&gt;https://www.swebench.com/&lt;/a&gt;&lt;/p&gt;&#xA;&lt;p&gt;&lt;a href=&#34;https://linzhiqiu.github.io/papers/naturalbench/?fbclid=IwY2xjawJ1xCpleHRuA2FlbQIxMQABHnFZ6hln8p75Kuz4l9F4Mgow7kzEgS1GKuRYj6q-DlvUAWVVRiyVmW1SvnwQ_aem_y_RMPY4cokQHJk8TpxQwpQ&#34;&gt;https://linzhiqiu.github.io/papers/naturalbench/?fbclid=IwY2xjawJ1xCpleHRuA2FlbQIxMQABHnFZ6hln8p75Kuz4l9F4Mgow7kzEgS1GKuRYj6q-DlvUAWVVRiyVmW1SvnwQ_aem_y_RMPY4cokQHJk8TpxQwpQ&lt;/a&gt;&lt;/p&gt;&#xA;&lt;p&gt;&lt;a href=&#34;https://github.com/Baiqi-Li/NaturalBench&#34;&gt;https://github.com/Baiqi-Li/NaturalBench&lt;/a&gt;&lt;/p&gt;&#xA;&lt;p&gt;trackingAI.org&lt;/p&gt;&#xA;&lt;p&gt;&lt;a href=&#34;https://llm-stats.com/&#34;&gt;https://llm-stats.com/&lt;/a&gt;&lt;/p&gt;&#xA;&lt;p&gt;&lt;a href=&#34;https://lmarena.ai/?leaderboard&#34;&gt;https://lmarena.ai/?leaderboard&lt;/a&gt;&lt;/p&gt;&#xA;&lt;p&gt;&lt;a href=&#34;https://fiction.live/stories/Fiction-liveBench-Feb-21-2025/oQdzQvKHw8JyXbN87&#34;&gt;https://fiction.live/stories/Fiction-liveBench-Feb-21-2025/oQdzQvKHw8JyXbN87&lt;/a&gt;&lt;/p&gt;&#xA;&lt;p&gt;&lt;a href=&#34;https://huggingface.co/spaces/adyen/DABstep&#34;&gt;Data Agent Benchmark&lt;/a&gt;&lt;/p&gt;&#xA;&lt;p&gt;&lt;a href=&#34;https://artificialanalysis.ai/&#34;&gt;https://artificialanalysis.ai/&lt;/a&gt;&lt;/p&gt;&#xA;&lt;p&gt;LiveCodeBench và SciCode&lt;/p&gt;&#xA;&lt;p&gt;&lt;a href=&#34;https://aider.chat/docs/leaderboards/?fbclid=IwY2xjawJs18lleHRuA2FlbQIxMQABHpMzr6OCU0YD65sAyMY5vDd4DKn00s4RKJniEUvlJIIeX4sIYMCtIq7MLZw8_aem_yF2JsBDjLMvkeFDhYfb-6A&#34;&gt;Aider polyglot&lt;/a&gt;&#xA;&lt;a href=&#34;https://github.com/Aider-AI/aider&#34;&gt;github&lt;/a&gt;&lt;/p&gt;&#xA;&lt;p&gt;&lt;a href=&#34;https://livebench.ai&#34;&gt;https://livebench.ai&lt;/a&gt;&#xA;&lt;a href=&#34;https://github.com/LiveBench/LiveBench&#34;&gt;https://github.com/LiveBench/LiveBench&lt;/a&gt;&lt;/p&gt;&#xA;&lt;p&gt;&lt;a href=&#34;https://openrouter.ai/rankings&#34;&gt;https://openrouter.ai/rankings&lt;/a&gt;&lt;/p&gt;&#xA;&lt;p&gt;&lt;a href=&#34;https://arcprize.org/leaderboard?fbclid=IwY2xjawJkGOJleHRuA2FlbQIxMAABHpInxwGwuzaVHnGeNNycEGfhmweu8Xb_aBq5dhGnOHLm1qEbktYZYnqZzNmc_aem_ttSWRTegPXjvOSU1K0DAlg&#34;&gt;https://arcprize.org/leaderboard?fbclid=IwY2xjawJkGOJleHRuA2FlbQIxMAABHpInxwGwuzaVHnGeNNycEGfhmweu8Xb_aBq5dhGnOHLm1qEbktYZYnqZzNmc_aem_ttSWRTegPXjvOSU1K0DAlg&lt;/a&gt;&lt;/p&gt;&#xA;&lt;p&gt;![[Pasted image 20250410103636.png]]&lt;/p&gt;&#xA;&lt;p&gt;EQ-Bench - Longform Creative Writing: &lt;a href=&#34;https://arxiv.org/pdf/2312.06281&#34;&gt;paper&lt;/a&gt;&#xA;![EQ-Bench][https://eqbench.com/images/eqbench3-judge-comparison.png]&lt;/p&gt;&#xA;&lt;p&gt;&lt;a href=&#34;https://llmbenchmark.kili-technology.com/?_gl=1&#34;&gt;https://llmbenchmark.kili-technology.com/?_gl=1&lt;/a&gt;&lt;em&gt;1y0re2j&lt;/em&gt;_gcl_au*NzA4OTAwNjM4LjE3NDQ5MjAzNDE.&lt;/p&gt;&#xA;&lt;figure&gt;&lt;img src=&#34;https://eqbench.com/images/eqbench3-judge-comparison.png&#34;&gt;&lt;figcaption&gt;&#xA;      &lt;h4&gt;Judge Comparison&lt;/h4&gt;&#xA;    &lt;/figcaption&gt;&#xA;&lt;/figure&gt;&#xA;&#xA;&lt;p&gt;Table of Contents&lt;/p&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;&lt;a href=&#34;https://kili-technology.com/large-language-models-llms/9-open-sourced-datasets-for-training-large-language-models#3&#34;&gt;Why are datasets important in training LLMs?&lt;/a&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;&lt;a href=&#34;https://kili-technology.com/large-language-models-llms/9-open-sourced-datasets-for-training-large-language-models#common-challenges-when-preparing-training-datasets&#34;&gt;Common challenges when preparing training datasets&lt;/a&gt;&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;/li&gt;&#xA;&lt;li&gt;&lt;a href=&#34;https://kili-technology.com/large-language-models-llms/9-open-sourced-datasets-for-training-large-language-models#27&#34;&gt;Popular Open Source Datasets for Training LLMs&lt;/a&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;&lt;a href=&#34;https://kili-technology.com/large-language-models-llms/9-open-sourced-datasets-for-training-large-language-models#1.-common-crawl&#34;&gt;1. Common Crawl&lt;/a&gt;&lt;/li&gt;&#xA;&lt;li&gt;&lt;a href=&#34;https://kili-technology.com/large-language-models-llms/9-open-sourced-datasets-for-training-large-language-models#2.-refinedweb&#34;&gt;2. RefinedWeb&lt;/a&gt;&lt;/li&gt;&#xA;&lt;li&gt;&lt;a href=&#34;https://kili-technology.com/large-language-models-llms/9-open-sourced-datasets-for-training-large-language-models#3.-the-pile&#34;&gt;3. The Pile&lt;/a&gt;&lt;/li&gt;&#xA;&lt;li&gt;&lt;a href=&#34;https://kili-technology.com/large-language-models-llms/9-open-sourced-datasets-for-training-large-language-models#4.-c4&#34;&gt;4. C4&lt;/a&gt;&lt;/li&gt;&#xA;&lt;li&gt;&lt;a href=&#34;https://kili-technology.com/large-language-models-llms/9-open-sourced-datasets-for-training-large-language-models#5.-starcoder-data&#34;&gt;5. Starcoder Data&lt;/a&gt;&lt;/li&gt;&#xA;&lt;li&gt;&lt;a href=&#34;https://kili-technology.com/large-language-models-llms/9-open-sourced-datasets-for-training-large-language-models#6.-bookcorpus&#34;&gt;6. BookCorpus&lt;/a&gt;&lt;/li&gt;&#xA;&lt;li&gt;&lt;a href=&#34;https://kili-technology.com/large-language-models-llms/9-open-sourced-datasets-for-training-large-language-models#7.-roots&#34;&gt;7. ROOTS&lt;/a&gt;&lt;/li&gt;&#xA;&lt;li&gt;&lt;a href=&#34;https://kili-technology.com/large-language-models-llms/9-open-sourced-datasets-for-training-large-language-models#8.-wikipedia&#34;&gt;8. Wikipedia&lt;/a&gt;&lt;/li&gt;&#xA;&lt;li&gt;&lt;a href=&#34;https://kili-technology.com/large-language-models-llms/9-open-sourced-datasets-for-training-large-language-models#9.-red-pajama&#34;&gt;9. Red Pajama&lt;/a&gt;&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;/li&gt;&#xA;&lt;li&gt;&lt;a href=&#34;https://kili-technology.com/large-language-models-llms/9-open-sourced-datasets-for-training-large-language-models#73&#34;&gt;Why Data Preprocessing is Important when Using Open Source Datasets&lt;/a&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;&lt;a href=&#34;https://kili-technology.com/large-language-models-llms/9-open-sourced-datasets-for-training-large-language-models#data-pre-processing-techniques&#34;&gt;Data Pre-Processing Techniques&lt;/a&gt;&lt;/li&gt;&#xA;&lt;li&gt;&lt;a href=&#34;https://kili-technology.com/large-language-models-llms/9-open-sourced-datasets-for-training-large-language-models#cleaning-and-normalization&#34;&gt;Cleaning and normalization&lt;/a&gt;&lt;/li&gt;&#xA;&lt;li&gt;&lt;a href=&#34;https://kili-technology.com/large-language-models-llms/9-open-sourced-datasets-for-training-large-language-models#tokenization-and-vectorization&#34;&gt;Tokenization and vectorization&lt;/a&gt;&lt;/li&gt;&#xA;&lt;li&gt;&lt;a href=&#34;https://kili-technology.com/large-language-models-llms/9-open-sourced-datasets-for-training-large-language-models#handling-of-missing-data&#34;&gt;Handling of missing data&lt;/a&gt;&lt;/li&gt;&#xA;&lt;li&gt;&lt;a href=&#34;https://kili-technology.com/large-language-models-llms/9-open-sourced-datasets-for-training-large-language-models#data-augmentation&#34;&gt;Data augmentation&lt;/a&gt;&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;/li&gt;&#xA;&lt;li&gt;&lt;a href=&#34;https://kili-technology.com/large-language-models-llms/9-open-sourced-datasets-for-training-large-language-models#115&#34;&gt;Ethical Considerations and Challenges When Applying Datasets in Machine Learning&lt;/a&gt;&lt;/li&gt;&#xA;&lt;li&gt;&lt;a href=&#34;https://kili-technology.com/large-language-models-llms/9-open-sourced-datasets-for-training-large-language-models#126&#34;&gt;How Datasets are used in Fine-Tuning and Evaluating LLMs&lt;/a&gt;&lt;/li&gt;&#xA;&lt;li&gt;&lt;a href=&#34;https://kili-technology.com/large-language-models-llms/9-open-sourced-datasets-for-training-large-language-models#142&#34;&gt;Conclusion: You Reap what you Sow&lt;/a&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;&lt;a href=&#34;https://kili-technology.com/large-language-models-llms/9-open-sourced-datasets-for-training-large-language-models#additional-reading&#34;&gt;Additional Reading&lt;/a&gt;&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;p&gt;The emergence of large language models (LLMs) sparks revolutionary transformation in various industries. While ChatGPT has impressed the public with its ingenious take on poetic writing, organizations are adopting deep learning AI models to build advanced neural information processing systems for specialized use cases.&lt;/p&gt;</description>
    </item>
    <item>
      <title>LLMtesting</title>
      <link>https://letungbach.com/posts/test/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://letungbach.com/posts/test/</guid>
      <description>&lt;p&gt;&lt;a href=&#34;https://www.promptfoo.dev/&#34;&gt;https://www.promptfoo.dev/&lt;/a&gt;&#xA;&lt;a href=&#34;https://github.com/promptfoo/promptfoo&#34;&gt;https://github.com/promptfoo/promptfoo&lt;/a&gt;&lt;/p&gt;</description>
    </item>
    <item>
      <title>mcp</title>
      <link>https://letungbach.com/posts/mcp/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://letungbach.com/posts/mcp/</guid>
      <description>&lt;p&gt;opensource MCP:&#xA;&lt;a href=&#34;https://github.com/pietrozullo/mcp-use&#34;&gt;https://github.com/pietrozullo/mcp-use&lt;/a&gt;&lt;/p&gt;&#xA;&lt;p&gt;![[Pasted image 20250414230045.png]]![[Pasted image 20250414230109.png]]&lt;/p&gt;&#xA;&lt;p&gt;![[Pasted image 20250416205343.png]]&lt;/p&gt;&#xA;&lt;p&gt;  &amp;ldquo;my-mcp-server-fdbeb09b&amp;rdquo;: {&lt;/p&gt;&#xA;&lt;p&gt;                &amp;ldquo;type&amp;rdquo;: &amp;ldquo;stdio&amp;rdquo;,&lt;/p&gt;&#xA;&lt;p&gt;                &amp;ldquo;command&amp;rdquo;: &amp;ldquo;@modelcontextprotocol/docker-mcp&amp;rdquo;,&lt;/p&gt;&#xA;&lt;p&gt;                &amp;ldquo;args&amp;rdquo;: []&lt;/p&gt;&#xA;&lt;p&gt;            }&lt;/p&gt;</description>
    </item>
  </channel>
</rss>
